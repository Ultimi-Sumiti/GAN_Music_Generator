# Symbolic-Domain Music Generation with GANs

![Music Generation](https://img.shields.io/badge/domain-music%20generation-blue) 
![GANs](https://img.shields.io/badge/model-GANs-orange) 
![MIDI](https://img.shields.io/badge/data-MIDI-lightgrey)

This project explores symbolic-domain music generation using Generative Adversarial Networks (GANs), inspired by the [MidiNet paper](https://arxiv.org/abs/1703.10847). The goal is to generate melodies or full musical pieces from MIDI data.

---

## ğŸ‘¥ Team Members
- **Luca Piai**   [GitHub](https://github.com/luca037)   

- **Alessandro chinello** [GitHub](https://github.com/Ale10chine) 

- **Mattia Scantamburlo**  [GitHub](https://github.com/Daedalus02)  

---

## ğŸ“Œ Project Overview
### Symbolic-Domain Music Generation
Music generation involves creating musical pieces using neural networks. This project focuses on **symbolic representations** (e.g., MIDI files), which encode notes, instruments, tempo, and other metadata, rather than raw audio.

### MIDI Files
- **Structure**: Split into channels (one per instrument), each containing a *piano roll* (ordered notes with duration/velocity).  
- **Piano Roll**: Represented as an `M Ã— N` matrix, where:  
  - `M` = Number of notes (low to high pitches).  
  - `N` = Number of timesteps.  

---

## ğŸ“‚ Project Structure

gan-project/
â”‚
â”œâ”€â”€ ğŸ“ data/ # Raw/processed data (excluded from version control)
â”‚ â”œâ”€â”€ raw/ # Original datasets (e.g., MIDI files)
â”‚ â””â”€â”€ processed/ # Preprocessed data for training
â”‚
â”œâ”€â”€ ğŸ“ models/ # Model architectures
â”‚ â”œâ”€â”€ generator.py # GAN generator
â”‚ â””â”€â”€ discriminator.py # GAN discriminator
â”‚
â”œâ”€â”€ ğŸ“ utils/ # Utilities
â”‚ â”œâ”€â”€ dataset_loader.py # MIDI data loading
â”‚ â”œâ”€â”€ metrics.py # Evaluation metrics
â”‚ â””â”€â”€ plot_tools.py # Visualization tools
â”‚
â”œâ”€â”€ ğŸ“ configs/ # Training configurations (YAML/JSON)
â”‚ â””â”€â”€ default.yaml # Hyperparameters
â”‚
â”œâ”€â”€ ğŸ“ training/ # Training scripts
â”‚ â”œâ”€â”€ train.py # Main training loop
â”‚ â””â”€â”€ trainer.py # Training logic
â”‚
â”œâ”€â”€ ğŸ“ evaluation/ # Model evaluation
â”‚ â””â”€â”€ evaluate.py # Metrics/comparisons
â”‚
â”œâ”€â”€ ğŸ“ experiments/ # Experiment logs/notebooks
â”‚ â””â”€â”€ exp1_gan_vs_wgan.ipynb # Example experiment
â”‚
â”œâ”€â”€ ğŸ“ outputs/ # Generated outputs
â”‚ â”œâ”€â”€ images/ # Sample outputs
â”‚ â”œâ”€â”€ checkpoints/ # Saved models
â”‚ â””â”€â”€ logs/ # Training logs
â”‚
â”œâ”€â”€ .gitignore # Excluded files (e.g., large datasets)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ main.py # Entry point for training



---

## ğŸµ Datasets
Suggested MIDI datasets:  
- **[Lakh MIDI Dataset](https://colinraffel.com/projects/lmd/)**: 176K+ MIDI files (use the *Clean-MIDI subset*).  
- **[MAESTRO Dataset](https://magenta.tensorflow.org/datasets/maestro)**: 200+ hours of piano performances.  

---

## ğŸ¯ Tasks (Ordered by Difficulty)
1. **Melody generation** (single-note sequences).  
2. **Melody generation conditioned on previous notes**.  
3. **Melody generation conditioned on chords**.  
4. **Full musical piece generation** (melody + chords).  

---

## ğŸ’¡ Suggestions
- **Preprocessing**: Spend time exploring MIDI data (e.g., isolating melodies/chords).  
- **Architectures**: Experiment with CNNs (e.g., ResNet, Inception) or VAEs.  
- **Music Theory**: Basic knowledge helps interpret data/literature.  

---

## ğŸ¼ Musical Terminology
- **Melody**: Sequence of single notes over time.  
- **Chords**: Multiple notes played simultaneously.  
- **Harmony**: Chord progression underlying a melody.  

Example:  
- **Chords**: `C Major â†’ G Major â†’ A Minor`.  
- **Melody**: `C â†’ E â†’ G â†’ A`.  

---

## ğŸ”§ Setup
1. Clone the repo:  
   ```bash
   git clone https://github.com/Ultimi-Sumiti/DL_project/gan-project.git

2. Install dependencies:
   ```bash  
   pip install -r requirements.txt

   
